{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443bde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9ad8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71f8aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_excel_files = glob(\"english_comments/*.xlsx\")\n",
    "\n",
    "df = pd.concat([pd.read_excel(excel_file,index_col=[0]) for excel_file in all_excel_files],ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f30fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = df['body'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9322ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloaded Amazon_book_reviews from Kaggle\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "with zipfile.ZipFile(\"archive.zip\", \"r\") as f:\n",
    "    for name in f.namelist():\n",
    "        #print(name)\n",
    "        if name=='Books_rating.csv':\n",
    "            with f.open(name) as zd:\n",
    "                amazon_reviews = pd.read_csv(zd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0503c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample is chosen for training the model\n",
    "amazon_sample=amazon_reviews.sample(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609997fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the text\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs, numbers, and special characters\n",
    "    text = re.sub(r'http\\\\S+|www\\\\.\\\\S+', ' ', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\\\s]', ' ', text)        # Remove special characters and numbers\n",
    "    text = re.sub(r'\\\\s+', ' ', text).strip() \n",
    "    # Remove extra whitespace\n",
    "    # Tokenize and lemmatize\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the datasets\n",
    "amazon_sample['review/text'] = amazon_sample['review/text'].apply(preprocess_text)\n",
    "df['body_c'] = df['body'].apply(preprocess_text)\n",
    "\n",
    "#remove non-ascii characters\n",
    "amazon_sample['review/text']=amazon_sample['review/text'].str.replace(r'\\\\x[0-9a-f]{2}', '', regex=True)\n",
    "df['body_c']=df['body_c'].str.replace(r'\\\\x[0-9a-f]{2}', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81262d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a98c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae027f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Map ratings to sentiment\n",
    "def map_sentiment(rating):\n",
    "    if rating <= 2:\n",
    "        return 0  # Negative\n",
    "    elif rating == 3:\n",
    "        return 1  # Neutral\n",
    "    else:\n",
    "        return 2  # Positive\n",
    "\n",
    "# Process the data\n",
    "amazon_sample['sentiment'] = amazon_sample['review/score'].apply(map_sentiment)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    amazon_sample['review/text'],\n",
    "    amazon_sample['sentiment'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=amazon_sample['sentiment']\n",
    ")\n",
    "\n",
    "# Prepare Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_train, 'label': y_train}))\n",
    "test_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_test, 'label': y_test}))\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the data\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=256)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Keep necessary columns only\n",
    "train_dataset = train_dataset.remove_columns(['text', '__index_level_0__'])\n",
    "test_dataset = test_dataset.remove_columns(['text', '__index_level_0__'])\n",
    "\n",
    "# Define a custom compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions, average='weighted'),\n",
    "        \"recall\": recall_score(labels, predictions, average='weighted'),\n",
    "        \"f1\": f1_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,  # Add compute_metrics for detailed evaluation\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "# Predict on Reddit comments\n",
    "reddit_comments_tokenized = tokenizer(\n",
    "    list(df['body_c']),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "reddit_comments_tokenized = {key: val.to(device) for key, val in reddit_comments_tokenized.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**reddit_comments_tokenized)\n",
    "predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "# Map predictions back to labels\n",
    "sentiment_labels = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "df['predicted_sentiment'] = [sentiment_labels[pred] for pred in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37149861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emotion extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_comment(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs, numbers, and special characters\n",
    "    text = re.sub(r'http\\\\S+|www\\\\.\\\\S+', ' ', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\\\s]', ' ', text)        # Remove special characters and numbers\n",
    "    text = re.sub(r'\\\\s+', ' ', text).strip() \n",
    "    \n",
    "df['body_c'] = df['body'].apply(preprocess_comment)\n",
    "\n",
    "#remove non-ascii characters\n",
    "df['body_c']=df['body_c'].str.replace(r'\\\\x[0-9a-f]{2}', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b351262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "\n",
    "def get_emotion_scores(text):\n",
    "    # Tokenize the text with padding and truncation\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Perform inference without updating gradients (for efficiency)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Apply softmax to the output logits to get probabilities\n",
    "    scores = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Map label indices to the corresponding emotion labels\n",
    "    emotion_labels = model.config.id2label\n",
    "    return {emotion_labels[i]: score.item() for i, score in enumerate(scores[0])}\n",
    "\n",
    "# Apply the function to the 'body_c' column to get emotion scores\n",
    "df['emotion_scores'] = df['body_c'].apply(get_emotion_scores)\n",
    "\n",
    "# Optionally, extract the top predicted emotion based on the highest score\n",
    "df['predicted_emotion'] = df['emotion_scores'].apply(lambda x: max(x, key=x.get))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
